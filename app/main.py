"""
ì§€í•˜ì²  í˜¼ì¡ë„ ì˜ˆì¸¡ - FastAPI ë°±ì—”ë“œ (ìˆ˜ì • ë²„ì „)
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional
import pandas as pd
import pickle
import numpy as np
from datetime import datetime
import os

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="ì§€í•˜ì²  í˜¼ì¡ë„ ì˜ˆì¸¡ API",
    description="ì„œìš¸ ì§€í•˜ì²  í˜¼ì¡ë„ë¥¼ ì˜ˆì¸¡í•˜ê³  ì‹¤ì‹œê°„ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” API",
    version="1.0.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ë³€ìˆ˜
model = None
feature_names = None
df_stations = None
df_features = None  # âœ… ì¶”ê°€!

# í˜¼ì¡ë„ ë ˆë²¨ ë§¤í•‘
CONGESTION_LEVEL_MAP = {
    0: "ì—¬ìœ ",
    1: "ë³´í†µ",
    2: "í˜¼ì¡",
    3: "ë§¤ìš°í˜¼ì¡"
}

CONGESTION_COLOR_MAP = {
    0: "#4CAF50",
    1: "#FFC107",
    2: "#FF9800",
    3: "#F44336"
}


# ìš”ì²­/ì‘ë‹µ ëª¨ë¸ ì •ì˜
class PredictionRequest(BaseModel):
    station_name: str
    line_name: str
    hour: int
    date: Optional[str] = None


class PredictionResponse(BaseModel):
    station_name: str
    line_name: str
    hour: int
    date: str
    congestion_level: int
    congestion_label: str
    congestion_color: str
    predicted_passengers: int
    confidence: float
    recommendation: str


class StationInfo(BaseModel):
    station_name: str
    line_name: str
    avg_passengers: int



@app.post("/debug/features")
async def debug_features(request: PredictionRequest):
    """ë””ë²„ê¹…: ì‹¤ì œ ì…ë ¥ë˜ëŠ” í”¼ì²˜ í™•ì¸"""
    date_str = request.date or datetime.now().strftime('%Y-%m-%d')
    
    try:
        features = prepare_features(
            request.station_name,
            request.line_name,
            request.hour,
            date_str
        )
        
        # í”¼ì²˜ë¥¼ ëª¨ë¸ ìˆœì„œë¡œ ì •ë ¬
        feature_values = [features[name] for name in feature_names]
        
        # í†µê³„
        import statistics
        
        return {
            "station": request.station_name,
            "line": request.line_name,
            "hour": request.hour,
            "feature_count": len(feature_values),
            "non_zero_features": sum(1 for v in feature_values if v != 0),
            "feature_stats": {
                "min": min(feature_values),
                "max": max(feature_values),
                "mean": statistics.mean(feature_values),
                "zeros": sum(1 for v in feature_values if v == 0)
            },
            "key_features": {
                "ì‹œê°„": features.get("ì‹œê°„"),
                "ìŠ¹ì°¨ì¸ì›": features.get("ìŠ¹ì°¨ì¸ì›"),
                "í•˜ì°¨ì¸ì›": features.get("í•˜ì°¨ì¸ì›"),
                "ì´ìŠ¹í•˜ì°¨ì¸ì›": features.get("ì´ìŠ¹í•˜ì°¨ì¸ì›"),
                "ì—­_í‰ê· ìŠ¹í•˜ì°¨": features.get("ì—­_í‰ê· ìŠ¹í•˜ì°¨"),
                "ì‹œê°„_í‰ê· ìŠ¹í•˜ì°¨": features.get("ì‹œê°„_í‰ê· ìŠ¹í•˜ì°¨"),
                "í˜¼ì¡ë„ë ˆë²¨": features.get("í˜¼ì¡ë„ë ˆë²¨")
            },
            "first_10_features": dict(list(features.items())[:10]),
            "model_expects": feature_names[:10]
        }
    except Exception as e:
        import traceback
        return {"error": str(e), "traceback": traceback.format_exc()}


@app.on_event("startup")
async def load_model():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë° ë°ì´í„° ë¡œë“œ"""
    global model, feature_names, df_stations, df_features
    
    print("ğŸš‡ ëª¨ë¸ ë° ë°ì´í„° ë¡œë”© ì¤‘...")
    
    try:
        # 1. ëª¨ë¸ ë¡œë“œ
        model_path = 'models/subway_congestion_model_improved.pkl'  # âœ… ìˆ˜ì •!
        with open(model_path, 'rb') as f:
            model_data = pickle.load(f)
            model = model_data['model']
            feature_names = model_data['feature_names']
        
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
        print(f"   í•„ìš”í•œ í”¼ì²˜ ìˆ˜: {len(feature_names)}")
        
        # 2. í”¼ì²˜ ë°ì´í„° ë¡œë“œ
        try:
            print("ğŸ“Š í”¼ì²˜ ë°ì´í„° ë¡œë”© ì¤‘...")
            df_features = pd.read_csv('data/processed/subway_features_balanced.csv', encoding='utf-8-sig')  # âœ… ìˆ˜ì •!
            print(f"âœ… í”¼ì²˜ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df_features):,}ê°œ ë ˆì½”ë“œ")
            print(f"   ì»¬ëŸ¼ ìˆ˜: {len(df_features.columns)}")
            
            # 3. ì—­ ëª©ë¡ ìƒì„± (ê°„ë‹¨í•˜ê²Œ)
            unique_stations = df_features[['ì§€í•˜ì² ì—­', 'í˜¸ì„ ëª…']].drop_duplicates()
            unique_stations = unique_stations.rename(columns={
                'ì§€í•˜ì² ì—­': 'station_name',
                'í˜¸ì„ ëª…': 'line_name'
            })
            df_stations = unique_stations
            
            print(f"âœ… ì—­ ì •ë³´ ìƒì„± ì™„ë£Œ: {len(df_stations)}ê°œ ì—­")
            
        except FileNotFoundError:
            print("âš ï¸  subway_features_balanced.csvë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")  # âœ… ë©”ì‹œì§€ë„ ìˆ˜ì •
            df_features = pd.DataFrame()
            df_stations = pd.DataFrame()
        except Exception as e:
            print(f"âš ï¸  ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            df_features = pd.DataFrame()
            df_stations = pd.DataFrame()
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise


def prepare_features(station_name: str, line_name: str, hour: int, date_str: str):
    """ì˜ˆì¸¡ì„ ìœ„í•œ í”¼ì²˜ ì¤€ë¹„ - ì‹œê°„ëŒ€ë³„ ì‹¤ì œ ë°ì´í„° ì‚¬ìš©"""
    global df_features, feature_names
    
    date = datetime.strptime(date_str, '%Y-%m-%d')
    
    if df_features is None or df_features.empty:
        raise HTTPException(status_code=500, detail="í”¼ì²˜ ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    # í•´ë‹¹ ì—­-ì‹œê°„ ì°¾ê¸°
    mask = (
        (df_features['ì§€í•˜ì² ì—­'] == station_name) & 
        (df_features['í˜¸ì„ ëª…'] == line_name) &
        (df_features['ì‹œê°„'] == hour)
    )
    
    matching_data = df_features[mask]
    
    # ìœ ì—°í•œ ê²€ìƒ‰
    if matching_data.empty:
        mask = (
            (df_features['ì§€í•˜ì² ì—­'].str.contains(station_name, na=False)) & 
            (df_features['í˜¸ì„ ëª…'].str.contains(line_name.replace('í˜¸ì„ ', ''), na=False))
        )
        all_station_data = df_features[mask]
        
        if all_station_data.empty:
            raise HTTPException(status_code=404, detail=f"{station_name} {line_name} ë°ì´í„° ì—†ìŒ")
        
        matching_data = all_station_data[all_station_data['ì‹œê°„'] == hour]
        
        if matching_data.empty:
            available_hours = all_station_data['ì‹œê°„'].unique()
            closest_hour = min(available_hours, key=lambda x: abs(x - hour))
            matching_data = all_station_data[all_station_data['ì‹œê°„'] == closest_hour]
    
    # ğŸ”¥ í•µì‹¬: í•´ë‹¹ ì‹œê°„ëŒ€ì˜ í‰ê· ê°’ ì‚¬ìš© (ì—¬ëŸ¬ ë‚ ì§œì˜ í‰ê· )
    # í•œ í–‰ë§Œ ì“°ì§€ ë§ê³ , í•´ë‹¹ ì‹œê°„ëŒ€ ì „ì²´ í‰ê· !
    time_avg_data = matching_data.mean(numeric_only=True)
    
    # íƒ€ê²Ÿ ë³€ìˆ˜ ì œì™¸
    EXCLUDE_FEATURES = [
        'í˜¼ì¡ë„ë ˆë²¨', 'í˜¼ì¡ë„', 'ì´ìŠ¹í•˜ì°¨ì¸ì›',
        'ì‚¬ìš©ì¼ì', 'ì§€í•˜ì² ì—­', 'í˜¸ì„ ëª…', 'Unnamed: 0'
    ]
    
    features = {}
    
    for feature_name in feature_names:
        if feature_name in EXCLUDE_FEATURES:
            continue
            
        if feature_name in time_avg_data.index:
            value = time_avg_data[feature_name]
            features[feature_name] = 0.0 if pd.isna(value) else float(value)
        else:
            features[feature_name] = 0.0
    
    # ë‚ ì§œ ê´€ë ¨ í”¼ì²˜ ì—…ë°ì´íŠ¸ (í˜„ì¬ ë‚ ì§œ)
    features['ìš”ì¼'] = float(date.weekday())
    features['ì›”'] = float(date.month)
    features['ì—°ë„'] = float(date.year)
    features['ë¶„ê¸°'] = float((date.month - 1) // 3 + 1)
    features['ì£¼ë§ì—¬ë¶€'] = float(1 if date.weekday() >= 5 else 0)
    features['ì—°íœ´ì—¬ë¶€'] = features['ì£¼ë§ì—¬ë¶€']
    features['ì‹œê°„'] = float(hour)
    
    # sin/cos ì—…ë°ì´íŠ¸
    features['ì‹œê°„_sin'] = np.sin(2 * np.pi * hour / 24)
    features['ì‹œê°„_cos'] = np.cos(2 * np.pi * hour / 24)
    features['ìš”ì¼_sin'] = np.sin(2 * np.pi * date.weekday() / 7)
    features['ìš”ì¼_cos'] = np.cos(2 * np.pi * date.weekday() / 7)
    features['ì›”_sin'] = np.sin(2 * np.pi * date.month / 12)
    features['ì›”_cos'] = np.cos(2 * np.pi * date.month / 12)
    
    return features

@app.get("/")
async def root():
    return {
        "message": "ì§€í•˜ì²  í˜¼ì¡ë„ ì˜ˆì¸¡ API",
        "version": "1.0.0",
        "docs": "/docs"
    }


@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "data_loaded": df_features is not None and not df_features.empty
    }


@app.get("/stations", response_model=List[StationInfo])
async def get_stations():
    if df_stations is None or df_stations.empty:
        raise HTTPException(status_code=500, detail="ì—­ ì •ë³´ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    return df_stations.head(100).to_dict('records')


@app.post("/predict", response_model=PredictionResponse)
async def predict_congestion(request: PredictionRequest):
    if model is None:
        raise HTTPException(status_code=500, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    date_str = request.date or datetime.now().strftime('%Y-%m-%d')
    
    if not 0 <= request.hour <= 23:
        raise HTTPException(status_code=400, detail="ì‹œê°„ì€ 0~23 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤")
    
    try:
        features = prepare_features(
            request.station_name,
            request.line_name,
            request.hour,
            date_str
        )
        
        feature_values = [features[name] for name in feature_names]
        
        # ëª¨ë¸ ì˜ˆì¸¡
        X = np.array([feature_values])
        prediction = model.predict(X)[0]
        probability = model.predict_proba(X)[0]
        confidence = float(probability[prediction])
        
        # ğŸ”¥ í•µì‹¬: ì—­ í‰ê·  ëŒ€ë¹„ ë¹„ìœ¨ ê³„ì‚°
        actual_passengers = features.get('ìŠ¹ì°¨ì¸ì›', 0) + features.get('í•˜ì°¨ì¸ì›', 0)
        station_avg = features.get('ì—­_í‰ê· ìŠ¹í•˜ì°¨', 30000)
        
        # ìƒëŒ€ì  í˜¼ì¡ë„ = í˜„ì¬ ìŠ¹í•˜ì°¨ / ì—­ í‰ê· 
        relative_congestion = actual_passengers / station_avg if station_avg > 0 else 1.0
        
        date = datetime.strptime(date_str, '%Y-%m-%d')
        is_weekend = date.weekday() >= 5
        
        # ğŸ¯ ìƒëŒ€ì  ë¹„ìœ¨ ê¸°ë°˜ í˜¼ì¡ë„ íŒë‹¨
        if relative_congestion < 0.4:
            adjusted_prediction = 0  # ì—¬ìœ  (í‰ê· ì˜ 40% ë¯¸ë§Œ)
        elif relative_congestion < 0.8:
            adjusted_prediction = 1  # ë³´í†µ (í‰ê· ì˜ 40~80%)
        elif relative_congestion < 1.3:
            adjusted_prediction = 2  # í˜¼ì¡ (í‰ê· ì˜ 80~130%)
        else:
            adjusted_prediction = 3  # ë§¤ìš°í˜¼ì¡ (í‰ê· ì˜ 130% ì´ìƒ)
        
        # ì‹œê°„ëŒ€ë³„ ì¶”ê°€ ë³´ì •
        if request.hour <= 5 or request.hour >= 23:
            adjusted_prediction = min(adjusted_prediction, 1)  # ì‹¬ì•¼ëŠ” ìµœëŒ€ "ë³´í†µ"
        
        # ì£¼ë§ ë³´ì •
        if is_weekend and adjusted_prediction >= 3:
            adjusted_prediction = 2  # ì£¼ë§ì€ ìµœëŒ€ "í˜¼ì¡"
        
        # ëª¨ë¸ ì˜ˆì¸¡ê³¼ ê·œì¹™ ê¸°ë°˜ ì˜ˆì¸¡ ì¤‘ ë” ë³´ìˆ˜ì ì¸ ê²ƒ ì„ íƒ
        # (ì•ˆì „ì„ ìœ„í•´ ë” í˜¼ì¡í•œ ìª½ ì„ íƒ)
        final_prediction = max(int(prediction), adjusted_prediction)
        
        # ì˜ˆìƒ ìŠ¹í•˜ì°¨ ì¸ì›
        predicted_passengers = int(actual_passengers)
        
        # ì¶”ì²œ ë©”ì‹œì§€ (ìƒëŒ€ì  í˜¼ì¡ë„ ì •ë³´ í¬í•¨)
        congestion_percent = int(relative_congestion * 100)
        
        base_recommendations = {
            0: f"ì—¬ìœ ë¡œì›Œìš”! (í‰ê· ì˜ {congestion_percent}%) ì§€ê¸ˆ ë°”ë¡œ ì´ìš©í•˜ì„¸ìš” ğŸ˜Š",
            1: f"ë³´í†µ ìˆ˜ì¤€ì´ì—ìš”. (í‰ê· ì˜ {congestion_percent}%) í¸í•˜ê²Œ ì´ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤ ğŸ‘",
            2: f"ë‹¤ì†Œ í˜¼ì¡í•´ìš”. (í‰ê· ì˜ {congestion_percent}%) ì‹œê°„ ì—¬ìœ ê°€ ìˆë‹¤ë©´ ë‹¤ë¥¸ ì‹œê°„ì„ ê³ ë ¤í•˜ì„¸ìš” âš ï¸",
            3: f"ë§¤ìš° í˜¼ì¡í•´ìš”! (í‰ê· ì˜ {congestion_percent}%) ê°€ëŠ¥í•˜ë©´ ë‹¤ë¥¸ ì‹œê°„ëŒ€ë¥¼ ì´ìš©í•˜ì„¸ìš” ğŸš«"
        }
        
        return PredictionResponse(
            station_name=request.station_name,
            line_name=request.line_name,
            hour=request.hour,
            date=date_str,
            congestion_level=final_prediction,
            congestion_label=CONGESTION_LEVEL_MAP[final_prediction],
            congestion_color=CONGESTION_COLOR_MAP[final_prediction],
            predicted_passengers=predicted_passengers,
            confidence=round(confidence, 2),
            recommendation=base_recommendations[final_prediction]
        )
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜: {str(e)}")


@app.get("/predict/hourly/{station_name}/{line_name}")
async def predict_hourly(station_name: str, line_name: str, date: Optional[str] = None):
    if model is None:
        raise HTTPException(status_code=500, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    date = date or datetime.now().strftime('%Y-%m-%d')
    results = []
    
    for hour in range(5, 24):
        try:
            features = prepare_features(station_name, line_name, hour, date)
            feature_values = [features[name] for name in feature_names]
            
            X = np.array([feature_values])
            prediction = model.predict(X)[0]
            
            results.append({
                "hour": hour,
                "congestion_level": int(prediction),
                "congestion_label": CONGESTION_LEVEL_MAP[prediction],
                "congestion_color": CONGESTION_COLOR_MAP[prediction]
            })
        except:
            continue
    
    return {
        "station_name": station_name,
        "line_name": line_name,
        "date": date,
        "hourly_predictions": results
    }


@app.get("/stations/search/{query}")
async def search_stations(query: str):
    if df_stations is None or df_stations.empty:
        raise HTTPException(status_code=500, detail="ì—­ ì •ë³´ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    results = df_stations[
        df_stations['station_name'].str.contains(query, case=False, na=False)
    ].head(20)
    
    return results.to_dict('records')


if __name__ == "__main__":
    import uvicorn
    
    print("ğŸš‡ ì§€í•˜ì²  í˜¼ì¡ë„ ì˜ˆì¸¡ API ì„œë²„ ì‹œì‘")
    print("=" * 60)
    print("ğŸ“ ì„œë²„ ì£¼ì†Œ: http://localhost:8000")
    print("ğŸ“š API ë¬¸ì„œ: http://localhost:8000/docs")
    print("=" * 60)
    
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)