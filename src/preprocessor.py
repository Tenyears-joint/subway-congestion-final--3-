"""
ì§€í•˜ì²  í˜¼ì¡ë„ ì˜ˆì¸¡ - ë°ì´í„° ì „ì²˜ë¦¬ ëª¨ë“ˆ (ì‹œê°„ëŒ€ë³„ ë²„ì „)
subway-congestion-prediction/src/preprocessor.py

ì£¼ìš” ê¸°ëŠ¥:
1. ì™€ì´ë“œ í¬ë§· â†’ ë¡± í¬ë§· ë³€í™˜ (ì‹œê°„ëŒ€ë³„)
2. ë‚ ì§œ/ì‹œê°„ ë°ì´í„° ì²˜ë¦¬
3. ê²°ì¸¡ì¹˜ ì²˜ë¦¬
4. ì´ìƒì¹˜ íƒì§€ ë° ì²˜ë¦¬
5. í˜¼ì¡ë„ ë ˆë²¨ ë¶„ë¥˜
"""

import pandas as pd
import numpy as np
from datetime import datetime
import os
import warnings
warnings.filterwarnings('ignore')


class SubwayDataPreprocessor:
    """ì§€í•˜ì²  ìŠ¹í•˜ì°¨ ë°ì´í„° ì „ì²˜ë¦¬ í´ë˜ìŠ¤ (ì‹œê°„ëŒ€ë³„)"""
    
    def __init__(self, raw_data_path, processed_data_path):
        """
        Args:
            raw_data_path: ì›ë³¸ ë°ì´í„° ê²½ë¡œ (data/raw/subway)
            processed_data_path: ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ê²½ë¡œ (data/processed/)
        """
        self.raw_data_path = raw_data_path
        self.processed_data_path = processed_data_path
        
        # í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
        os.makedirs(processed_data_path, exist_ok=True)
        
        self.df = None
        self.df_long = None
        
    def load_data(self, filename='ì„œìš¸ì‹œ ì§€í•˜ì²  í˜¸ì„ ë³„ ì—­ë³„ ì‹œê°„ëŒ€ë³„ ìŠ¹í•˜ì°¨ ì¸ì› ì •ë³´ (1).csv'):
        """ì›ë³¸ ë°ì´í„° ë¡œë“œ"""
        filepath = os.path.join(self.raw_data_path, filename)
        
        print(f"ğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘: {filepath}")
        
        # cp949 ì¸ì½”ë”©ìœ¼ë¡œ ì½ê¸°
        self.df = pd.read_csv(filepath, encoding='cp949')
        
        print(f"âœ… ë¡œë”© ì™„ë£Œ!")
        print(f"   - í–‰ ìˆ˜: {len(self.df):,}")
        print(f"   - ì—´ ìˆ˜: {len(self.df.columns)}")
        print(f"   - ë©”ëª¨ë¦¬: {self.df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
        
        return self.df
    
    def check_data_quality(self):
        """ë°ì´í„° í’ˆì§ˆ ì²´í¬"""
        print("\nğŸ” ë°ì´í„° í’ˆì§ˆ ì²´í¬")
        print("=" * 60)
        
        # 1. ê²°ì¸¡ì¹˜ í™•ì¸
        missing = self.df.isnull().sum()
        if missing.sum() > 0:
            print("\nâš ï¸  ê²°ì¸¡ì¹˜ ë°œê²¬:")
            print(missing[missing > 0].head(10))
        else:
            print("âœ… ê²°ì¸¡ì¹˜ ì—†ìŒ")
        
        # 2. ì¤‘ë³µ ë°ì´í„° í™•ì¸
        duplicates = self.df.duplicated(subset=['ì‚¬ìš©ì›”', 'í˜¸ì„ ëª…', 'ì§€í•˜ì² ì—­']).sum()
        print(f"\nì¤‘ë³µ í–‰: {duplicates}ê°œ")
        
        # 3. ì‚¬ìš©ì›” ë²”ìœ„ í™•ì¸
        print(f"\nì‚¬ìš©ì›” ë²”ìœ„: {self.df['ì‚¬ìš©ì›”'].min()} ~ {self.df['ì‚¬ìš©ì›”'].max()}")
        
        # 4. ë…¸ì„  ë° ì—­ ê°œìˆ˜
        print(f"\ní˜¸ì„  ìˆ˜: {self.df['í˜¸ì„ ëª…'].nunique()}ê°œ")
        print(f"ì—­ ìˆ˜: {self.df['ì§€í•˜ì² ì—­'].nunique()}ê°œ")
        
        return self.df
    
    def wide_to_long(self):
        """ì™€ì´ë“œ í¬ë§· â†’ ë¡± í¬ë§· ë³€í™˜ (ì‹œê°„ëŒ€ë³„)"""
        print("\nğŸ”„ ë°ì´í„° í˜•íƒœ ë³€í™˜ ì¤‘ (Wide â†’ Long)")
        
        # ì‹œê°„ëŒ€ ìŠ¹ì°¨ ì»¬ëŸ¼ ì¶”ì¶œ
        boarding_cols = [col for col in self.df.columns if 'ìŠ¹ì°¨ì¸ì›' in col]
        alighting_cols = [col for col in self.df.columns if 'í•˜ì°¨ì¸ì›' in col]
        
        print(f"   - ì‹œê°„ëŒ€ ìˆ˜: {len(boarding_cols)}ê°œ")
        
        # ìŠ¹ì°¨ ë°ì´í„° ë¡± í¬ë§· ë³€í™˜
        df_boarding = pd.melt(
            self.df,
            id_vars=['ì‚¬ìš©ì›”', 'í˜¸ì„ ëª…', 'ì§€í•˜ì² ì—­'],
            value_vars=boarding_cols,
            var_name='ì‹œê°„ëŒ€',
            value_name='ìŠ¹ì°¨ì¸ì›'
        )
        
        # í•˜ì°¨ ë°ì´í„° ë¡± í¬ë§· ë³€í™˜
        df_alighting = pd.melt(
            self.df,
            id_vars=['ì‚¬ìš©ì›”', 'í˜¸ì„ ëª…', 'ì§€í•˜ì² ì—­'],
            value_vars=alighting_cols,
            var_name='ì‹œê°„ëŒ€',
            value_name='í•˜ì°¨ì¸ì›'
        )
        
        # ì‹œê°„ëŒ€ì—ì„œ ì‹œê°„ ì¶”ì¶œ (04ì‹œ-05ì‹œ â†’ 04)
        df_boarding['ì‹œê°„'] = df_boarding['ì‹œê°„ëŒ€'].str.extract(r'(\d+)ì‹œ-').astype(int)
        df_alighting['ì‹œê°„'] = df_alighting['ì‹œê°„ëŒ€'].str.extract(r'(\d+)ì‹œ-').astype(int)
        
        # ìŠ¹ì°¨/í•˜ì°¨ ë°ì´í„° ë³‘í•©
        self.df_long = df_boarding.merge(
            df_alighting[['ì‚¬ìš©ì›”', 'í˜¸ì„ ëª…', 'ì§€í•˜ì² ì—­', 'ì‹œê°„', 'í•˜ì°¨ì¸ì›']],
            on=['ì‚¬ìš©ì›”', 'í˜¸ì„ ëª…', 'ì§€í•˜ì² ì—­', 'ì‹œê°„'],
            how='left'
        )
        
        # ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°
        self.df_long = self.df_long.drop('ì‹œê°„ëŒ€', axis=1)
        
        # ì´ ìŠ¹í•˜ì°¨ ì¸ì› ê³„ì‚°
        self.df_long['ì´ìŠ¹í•˜ì°¨ì¸ì›'] = self.df_long['ìŠ¹ì°¨ì¸ì›'] + self.df_long['í•˜ì°¨ì¸ì›']
        
        print(f"âœ… ë³€í™˜ ì™„ë£Œ!")
        print(f"   - ë³€í™˜ í›„ í–‰ ìˆ˜: {len(self.df_long):,}")
        
        return self.df_long
    
    def add_time_features(self):
        """ë‚ ì§œ/ì‹œê°„ ê´€ë ¨ í”¼ì²˜ ì¶”ê°€"""
        print("\nğŸ“… ì‹œê°„ í”¼ì²˜ ìƒì„± ì¤‘...")
        
        # ì‚¬ìš©ì›”ì„ ë‚ ì§œë¡œ ë³€í™˜ (202510 â†’ 2025-10-01)
        self.df_long['ì‚¬ìš©ì¼ì'] = pd.to_datetime(self.df_long['ì‚¬ìš©ì›”'].astype(str) + '01', format='%Y%m%d')
        
        # ë‚ ì§œ ì •ë³´ ì¶”ì¶œ
        self.df_long['ì—°ë„'] = self.df_long['ì‚¬ìš©ì¼ì'].dt.year
        self.df_long['ì›”'] = self.df_long['ì‚¬ìš©ì¼ì'].dt.month
        
        # ì‹œê°„ëŒ€ êµ¬ë¶„ (ì¶œí‡´ê·¼ ì‹œê°„, ì‹¬ì•¼ ë“±)
        def classify_time_period(hour):
            if 7 <= hour <= 9:
                return 'ì¶œê·¼ì‹œê°„'
            elif 18 <= hour <= 20:
                return 'í‡´ê·¼ì‹œê°„'
            elif 11 <= hour <= 13:
                return 'ì ì‹¬ì‹œê°„'
            elif 22 <= hour or hour <= 5:
                return 'ì‹¬ì•¼ì‹œê°„'
            else:
                return 'ì¼ë°˜ì‹œê°„'
        
        self.df_long['ì‹œê°„ëŒ€êµ¬ë¶„'] = self.df_long['ì‹œê°„'].apply(classify_time_period)
        
        # í˜¼ì¡ë„ ë ˆë²¨ ë¶„ë¥˜ (4ë‹¨ê³„)
        def classify_congestion(passengers):
            if passengers < 500:
                return 'ì—¬ìœ '
            elif passengers < 1500:
                return 'ë³´í†µ'
            elif passengers < 3000:
                return 'í˜¼ì¡'
            else:
                return 'ë§¤ìš°í˜¼ì¡'
        
        self.df_long['í˜¼ì¡ë„'] = self.df_long['ì´ìŠ¹í•˜ì°¨ì¸ì›'].apply(classify_congestion)
        
        # í˜¼ì¡ë„ ë ˆë²¨ (ìˆ«ì)
        congestion_map = {'ì—¬ìœ ': 0, 'ë³´í†µ': 1, 'í˜¼ì¡': 2, 'ë§¤ìš°í˜¼ì¡': 3}
        self.df_long['í˜¼ì¡ë„ë ˆë²¨'] = self.df_long['í˜¼ì¡ë„'].map(congestion_map)
        
        print("âœ… ì‹œê°„ í”¼ì²˜ ì¶”ê°€ ì™„ë£Œ!")
        print(f"   - ì¶”ê°€ëœ í”¼ì²˜: ì‚¬ìš©ì¼ì, ì—°ë„, ì›”, ì‹œê°„ëŒ€êµ¬ë¶„, í˜¼ì¡ë„, í˜¼ì¡ë„ë ˆë²¨")
        
        return self.df_long
    
    def handle_missing_values(self):
        """ê²°ì¸¡ì¹˜ ì²˜ë¦¬"""
        print("\nğŸ”§ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì¤‘...")
        
        before_count = len(self.df_long)
        
        # NaNì„ 0ìœ¼ë¡œ ëŒ€ì²´ (ìŠ¹í•˜ì°¨ ì¸ì›ì´ ì—†ëŠ” ê²½ìš°)
        self.df_long['ìŠ¹ì°¨ì¸ì›'] = self.df_long['ìŠ¹ì°¨ì¸ì›'].fillna(0)
        self.df_long['í•˜ì°¨ì¸ì›'] = self.df_long['í•˜ì°¨ì¸ì›'].fillna(0)
        self.df_long['ì´ìŠ¹í•˜ì°¨ì¸ì›'] = self.df_long['ì´ìŠ¹í•˜ì°¨ì¸ì›'].fillna(0)
        
        after_count = len(self.df_long)
        
        print(f"âœ… ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"   - ì²˜ë¦¬ ì „: {before_count:,}ê°œ")
        print(f"   - ì²˜ë¦¬ í›„: {after_count:,}ê°œ")
        
        return self.df_long
    
    def handle_outliers(self, method='iqr'):
        """ì´ìƒì¹˜ ì²˜ë¦¬"""
        print(f"\nğŸ”§ ì´ìƒì¹˜ ì²˜ë¦¬ ì¤‘ (ë°©ë²•: {method})...")
        
        for col in ['ìŠ¹ì°¨ì¸ì›', 'í•˜ì°¨ì¸ì›', 'ì´ìŠ¹í•˜ì°¨ì¸ì›']:
            Q1 = self.df_long[col].quantile(0.25)
            Q3 = self.df_long[col].quantile(0.75)
            IQR = Q3 - Q1
            
            upper_bound = Q3 + 1.5 * IQR
            
            # ìŒìˆ˜ ê°’ì€ 0ìœ¼ë¡œ ì²˜ë¦¬
            self.df_long[col] = self.df_long[col].clip(lower=0)
            
            # ê·¹ë‹¨ì ì¸ ì´ìƒì¹˜ë§Œ ì²˜ë¦¬ (ìƒí•œê°’ì˜ 2ë°° ì´ìƒ)
            outliers = (self.df_long[col] > upper_bound * 2)
            outlier_count = outliers.sum()
            
            # ì´ìƒì¹˜ëŠ” ìƒí•œê°’ìœ¼ë¡œ ëŒ€ì²´
            self.df_long.loc[outliers, col] = upper_bound
            
            if outlier_count > 0:
                print(f"   - {col}: {outlier_count:,}ê°œ ì²˜ë¦¬")
        
        print(f"âœ… ì´ìƒì¹˜ ì²˜ë¦¬ ì™„ë£Œ!")
        
        return self.df_long
    
    def add_station_statistics(self):
        """ì—­ë³„ í†µê³„ ì •ë³´ ì¶”ê°€ (í‰ê·  í˜¼ì¡ë„ ë“±)"""
        print("\nğŸ“Š ì—­ë³„ í†µê³„ ìƒì„± ì¤‘...")
        
        # ì—­ë³„ í‰ê·  ìŠ¹í•˜ì°¨ ì¸ì›
        station_avg = self.df_long.groupby('ì§€í•˜ì² ì—­')['ì´ìŠ¹í•˜ì°¨ì¸ì›'].mean().reset_index()
        station_avg.columns = ['ì§€í•˜ì² ì—­', 'ì—­_í‰ê· ìŠ¹í•˜ì°¨']
        
        # ì‹œê°„ëŒ€ë³„ í‰ê·  ìŠ¹í•˜ì°¨ ì¸ì›
        time_avg = self.df_long.groupby('ì‹œê°„')['ì´ìŠ¹í•˜ì°¨ì¸ì›'].mean().reset_index()
        time_avg.columns = ['ì‹œê°„', 'ì‹œê°„_í‰ê· ìŠ¹í•˜ì°¨']
        
        # í˜¸ì„ ë³„ í‰ê·  ìŠ¹í•˜ì°¨ ì¸ì›
        line_avg = self.df_long.groupby('í˜¸ì„ ëª…')['ì´ìŠ¹í•˜ì°¨ì¸ì›'].mean().reset_index()
        line_avg.columns = ['í˜¸ì„ ëª…', 'í˜¸ì„ _í‰ê· ìŠ¹í•˜ì°¨']
        
        # ì›ë³¸ ë°ì´í„°ì— ë³‘í•©
        self.df_long = self.df_long.merge(station_avg, on='ì§€í•˜ì² ì—­', how='left')
        self.df_long = self.df_long.merge(time_avg, on='ì‹œê°„', how='left')
        self.df_long = self.df_long.merge(line_avg, on='í˜¸ì„ ëª…', how='left')
        
        print("âœ… ì—­ë³„ í†µê³„ ì¶”ê°€ ì™„ë£Œ!")
        
        return self.df_long
    
    def normalize_names(self):
        """ì—­ëª…/í˜¸ì„ ëª… í‘œì¤€í™” (ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ì œê±°)"""
        print("\nğŸ”¤ ì—­ëª…/í˜¸ì„ ëª… í‘œì¤€í™” ì¤‘...")
        
        # ê³µë°± ì œê±°
        self.df_long['ì§€í•˜ì² ì—­'] = self.df_long['ì§€í•˜ì² ì—­'].str.strip()
        self.df_long['í˜¸ì„ ëª…'] = self.df_long['í˜¸ì„ ëª…'].str.strip()
        
        print(f"âœ… í‘œì¤€í™” ì™„ë£Œ!")
        
        return self.df_long
    
    def save_processed_data(self, filename='subway_processed.csv'):
        """ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥"""
        filepath = os.path.join(self.processed_data_path, filename)
        
        print(f"\nğŸ’¾ ì „ì²˜ë¦¬ ë°ì´í„° ì €ì¥ ì¤‘: {filepath}")
        
        self.df_long.to_csv(filepath, index=False, encoding='utf-8-sig')
        
        print(f"âœ… ì €ì¥ ì™„ë£Œ!")
        print(f"   - ìµœì¢… í–‰ ìˆ˜: {len(self.df_long):,}")
        print(f"   - íŒŒì¼ í¬ê¸°: {os.path.getsize(filepath) / 1024**2:.2f} MB")
        
        return filepath
    
    def get_summary(self):
        """ì „ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½"""
        print("\n" + "="*60)
        print("ğŸ“‹ ì „ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½")
        print("="*60)
        
        print(f"\nì´ ë°ì´í„° ìˆ˜: {len(self.df_long):,}ê°œ")
        print(f"ê¸°ê°„: {self.df_long['ì‚¬ìš©ì¼ì'].min()} ~ {self.df_long['ì‚¬ìš©ì¼ì'].max()}")
        print(f"í˜¸ì„  ìˆ˜: {self.df_long['í˜¸ì„ ëª…'].nunique()}ê°œ")
        print(f"ì—­ ìˆ˜: {self.df_long['ì§€í•˜ì² ì—­'].nunique()}ê°œ")
        print(f"ì‹œê°„ëŒ€ ìˆ˜: {self.df_long['ì‹œê°„'].nunique()}ê°œ")
        
        print("\nì´ìŠ¹í•˜ì°¨ ì¸ì› í†µê³„:")
        print(self.df_long['ì´ìŠ¹í•˜ì°¨ì¸ì›'].describe())
        
        print("\nì‹œê°„ëŒ€êµ¬ë¶„ë³„ í‰ê·  ìŠ¹í•˜ì°¨:")
        print(self.df_long.groupby('ì‹œê°„ëŒ€êµ¬ë¶„')['ì´ìŠ¹í•˜ì°¨ì¸ì›'].mean().sort_values(ascending=False))
        
        print("\ní˜¼ì¡ë„ ë¶„í¬:")
        print(self.df_long['í˜¼ì¡ë„'].value_counts().sort_index())
        
        print("\nìƒìœ„ 10ê°œ í˜¼ì¡ ì—­ (í‰ê· ):")
        top_stations = self.df_long.groupby('ì§€í•˜ì² ì—­')['ì´ìŠ¹í•˜ì°¨ì¸ì›'].mean().sort_values(ascending=False).head(10)
        print(top_stations)
        
        print("\nì»¬ëŸ¼ ëª©ë¡:")
        print(self.df_long.columns.tolist())


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸš‡ ì§€í•˜ì²  í˜¼ì¡ë„ ì˜ˆì¸¡ - ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘ (ì‹œê°„ëŒ€ë³„)")
    print("="*60)
    
    # ê²½ë¡œ ì„¤ì •
    RAW_DATA_PATH = 'data/raw/subway'
    PROCESSED_DATA_PATH = 'data/processed'
    
    # ì „ì²˜ë¦¬ ê°ì²´ ìƒì„±
    preprocessor = SubwayDataPreprocessor(RAW_DATA_PATH, PROCESSED_DATA_PATH)
    
    try:
        # 1. ë°ì´í„° ë¡œë“œ
        preprocessor.load_data()
        
        # 2. ë°ì´í„° í’ˆì§ˆ ì²´í¬
        preprocessor.check_data_quality()
        
        # 3. ì™€ì´ë“œ â†’ ë¡± í¬ë§· ë³€í™˜
        preprocessor.wide_to_long()
        
        # 4. ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        preprocessor.handle_missing_values()
        
        # 5. ì‹œê°„ í”¼ì²˜ ì¶”ê°€
        preprocessor.add_time_features()
        
        # 6. ì—­ëª…/í˜¸ì„ ëª… í‘œì¤€í™”
        preprocessor.normalize_names()
        
        # 7. ì´ìƒì¹˜ ì²˜ë¦¬
        preprocessor.handle_outliers()
        
        # 8. ì—­ë³„ í†µê³„ ì¶”ê°€
        preprocessor.add_station_statistics()
        
        # 9. ì €ì¥
        preprocessor.save_processed_data()
        
        # 10. ìš”ì•½
        preprocessor.get_summary()
        
        print("\nâœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()