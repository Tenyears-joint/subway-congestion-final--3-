"""
model_trainer_enhanced.py
ê°•í™”ëœ í”¼ì²˜ë¥¼ í¬í•¨í•œ ëª¨ë¸ í•™ìŠµ + ì‹œê°í™”
"""

import pandas as pd
import numpy as np
import pickle
from datetime import datetime
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Malgun Gothic'  # Windows
plt.rcParams['axes.unicode_minus'] = False  # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€

print("=" * 80)
print("ğŸš‚ ëª¨ë¸ í•™ìŠµ ì‹œì‘ (ê°•í™”ëœ í”¼ì²˜ í¬í•¨)")
print("=" * 80)

# 1. ë°ì´í„° ë¡œë“œ
print("\n1. ë°ì´í„° ë¡œë“œ ì¤‘...")
df = pd.read_csv('data/processed/subway_features_enhanced.csv', encoding='utf-8-sig')
print(f"   ë¡œë“œ ì™„ë£Œ: {len(df):,}ê°œ ë ˆì½”ë“œ, {len(df.columns)}ê°œ ì»¬ëŸ¼")

# 2. í”¼ì²˜ì™€ íƒ€ê²Ÿ ë¶„ë¦¬
print("\n2. í”¼ì²˜ì™€ íƒ€ê²Ÿ ë¶„ë¦¬ ì¤‘...")
TARGET = 'í˜¼ì¡ë„ë ˆë²¨'

EXCLUDE_COLUMNS = [
    TARGET,
    'ì‚¬ìš©ì¼ì', 'Unnamed: 0',
    'ì§€í•˜ì² ì—­', 'í˜¸ì„ ëª…'
]

feature_columns = [col for col in df.columns if col not in EXCLUDE_COLUMNS]
print(f"   í”¼ì²˜ ìˆ˜: {len(feature_columns)}ê°œ")

# ìƒˆë¡œìš´ í”¼ì²˜ í™•ì¸
category_features = [col for col in feature_columns if 'ì—­_' in col]
interaction_features = [col for col in feature_columns if 'interaction' in col]

if category_features:
    print(f"\n   ğŸ¯ ì—­ ì¹´í…Œê³ ë¦¬ í”¼ì²˜: {len(category_features)}ê°œ")
    for feat in category_features:
        print(f"      - {feat}")

if interaction_features:
    print(f"\n   âœ¨ ìƒí˜¸ì‘ìš© í”¼ì²˜: {len(interaction_features)}ê°œ")
    for feat in interaction_features:
        print(f"      - {feat}")

X = df[feature_columns]
y = df[TARGET]

print(f"\n   X shape: {X.shape}")
print(f"   y shape: {y.shape}")

# 3. ë°ì´í„° í’ˆì§ˆ í™•ì¸
print("\n3. ë°ì´í„° í’ˆì§ˆ í™•ì¸...")
print(f"   ê²°ì¸¡ì¹˜: {X.isnull().sum().sum()}ê°œ")
print(f"   ë¬´í•œëŒ€: {np.isinf(X.select_dtypes(include=[np.number])).sum().sum()}ê°œ")

# 4. í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬
print("\n4. í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬ ì¤‘...")
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2, 
    random_state=42,
    stratify=y
)

print(f"   í•™ìŠµ ë°ì´í„°: {X_train.shape[0]:,}ê°œ")
print(f"   í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape[0]:,}ê°œ")

# 5. ëª¨ë¸ í•™ìŠµ
print("\n5. Random Forest ëª¨ë¸ í•™ìŠµ ì¤‘...")
print("   (ì´ ê³¼ì •ì€ ëª‡ ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤...)")

model = RandomForestClassifier(
    n_estimators=100,
    max_depth=20,
    min_samples_split=50,
    min_samples_leaf=20,
    max_features='sqrt',
    random_state=42,
    n_jobs=-1,
    verbose=0
)

model.fit(X_train, y_train)
print("   âœ… í•™ìŠµ ì™„ë£Œ!")

# 6. ì˜ˆì¸¡ ë° í‰ê°€
print("\n6. ëª¨ë¸ í‰ê°€ ì¤‘...")

# í•™ìŠµ ë°ì´í„° í‰ê°€
y_train_pred = model.predict(X_train)
train_accuracy = accuracy_score(y_train, y_train_pred)
train_f1 = f1_score(y_train, y_train_pred, average='weighted')

# í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€
y_test_pred = model.predict(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_f1 = f1_score(y_test, y_test_pred, average='weighted')

print(f"\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥:")
print(f"   í•™ìŠµ ì •í™•ë„: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)")
print(f"   í•™ìŠµ F1-Score: {train_f1:.4f}")
print(f"   í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")
print(f"   í…ŒìŠ¤íŠ¸ F1-Score: {test_f1:.4f}")

# ê³¼ì í•© í™•ì¸
overfit = train_accuracy - test_accuracy
print(f"\n   ê³¼ì í•© ì •ë„: {overfit:.4f} ({overfit*100:.2f}%p)")
if overfit < 0.05:
    print("   âœ… ê³¼ì í•© ì—†ìŒ (5%p ë¯¸ë§Œ)")
elif overfit < 0.10:
    print("   âš ï¸ ì•½ê°„ ê³¼ì í•© (5-10%p)")
else:
    print("   âŒ ê³¼ì í•© ì‹¬ê° (10%p ì´ìƒ)")

# 7. ìƒì„¸ ë¦¬í¬íŠ¸
print(f"\n7. í´ë˜ìŠ¤ë³„ ì„±ëŠ¥:")
print("\n" + classification_report(y_test, y_test_pred, 
                                   target_names=['ì—¬ìœ ', 'ë³´í†µ', 'í˜¼ì¡', 'ë§¤ìš°í˜¼ì¡']))

# 8. Confusion Matrix
print(f"\n8. Confusion Matrix:")
cm = confusion_matrix(y_test, y_test_pred)
print(cm)

# models í´ë” ìƒì„±
import os
os.makedirs('models', exist_ok=True)

# ğŸ“Š í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
print(f"\n   ì‹œê°í™” ìƒì„± ì¤‘...")
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['ì—¬ìœ ', 'ë³´í†µ', 'í˜¼ì¡', 'ë§¤ìš°í˜¼ì¡'],
            yticklabels=['ì—¬ìœ ', 'ë³´í†µ', 'í˜¼ì¡', 'ë§¤ìš°í˜¼ì¡'],
            cbar_kws={'label': 'ìƒ˜í”Œ ìˆ˜'})
plt.title('í˜¼ë™ í–‰ë ¬ (Confusion Matrix)', fontsize=16, fontweight='bold', pad=20)
plt.xlabel('ì˜ˆì¸¡ê°’', fontsize=12)
plt.ylabel('ì‹¤ì œê°’', fontsize=12)

# ì •í™•ë„ë¥¼ ê° ì…€ì— ì¶”ê°€
for i in range(4):
    for j in range(4):
        total = cm[i].sum()
        percentage = cm[i, j] / total * 100 if total > 0 else 0
        plt.text(j + 0.5, i + 0.7, f'({percentage:.1f}%)', 
                ha='center', va='center', fontsize=9, color='gray')

plt.tight_layout()
plt.savefig('models/confusion_matrix_enhanced.png', dpi=300, bbox_inches='tight')
print(f"   âœ… ì €ì¥: models/confusion_matrix_enhanced.png")
plt.close()

# 9. Feature Importance
print(f"\n9. Feature Importance (ìƒìœ„ 20ê°œ):")
feature_importance = pd.DataFrame({
    'feature': feature_columns,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

print(feature_importance.head(20).to_string(index=False))

# ì—­ ì¹´í…Œê³ ë¦¬ í”¼ì²˜ ì¤‘ìš”ë„ í™•ì¸
if category_features:
    print(f"\n   ğŸ¯ ì—­ ì¹´í…Œê³ ë¦¬ í”¼ì²˜ ì¤‘ìš”ë„:")
    for feat in category_features:
        imp = feature_importance[feature_importance['feature'] == feat]['importance'].values
        if len(imp) > 0:
            print(f"      {feat}: {imp[0]:.4f}")

# ìƒí˜¸ì‘ìš© í”¼ì²˜ ì¤‘ìš”ë„ í™•ì¸
if interaction_features:
    print(f"\n   âœ¨ ìƒí˜¸ì‘ìš© í”¼ì²˜ ì¤‘ìš”ë„:")
    for feat in interaction_features:
        imp = feature_importance[feature_importance['feature'] == feat]['importance'].values
        if len(imp) > 0:
            print(f"      {feat}: {imp[0]:.4f}")

# ğŸ“Š íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™” (ìƒìœ„ 25ê°œ)
print(f"\n   ì‹œê°í™” ìƒì„± ì¤‘...")
top_n = 25
top_features = feature_importance.head(top_n)

plt.figure(figsize=(12, 10))

# ìƒ‰ìƒ êµ¬ë¶„: ì—­ ì¹´í…Œê³ ë¦¬(ë¹¨ê°•), ìƒí˜¸ì‘ìš©(ì£¼í™©), ê¸°ë³¸(ì²­ë¡)
colors = []
for feat in top_features['feature']:
    if 'ì—­_' in feat:
        colors.append('#FF6B6B')  # ë¹¨ê°• - ì—­ ì¹´í…Œê³ ë¦¬
    elif 'interaction' in feat:
        colors.append('#FFA500')  # ì£¼í™© - ìƒí˜¸ì‘ìš©
    else:
        colors.append('#4ECDC4')  # ì²­ë¡ - ê¸°ë³¸

bars = plt.barh(range(top_n), top_features['importance'], color=colors)
plt.yticks(range(top_n), top_features['feature'])
plt.xlabel('ì¤‘ìš”ë„ (Importance)', fontsize=12)
plt.title(f'íŠ¹ì„± ì¤‘ìš”ë„ Top {top_n}', fontsize=16, fontweight='bold', pad=20)
plt.gca().invert_yaxis()

# ë²”ë¡€ ì¶”ê°€
from matplotlib.patches import Patch
legend_elements = [
    Patch(facecolor='#FF6B6B', label='ì—­ ì¹´í…Œê³ ë¦¬ í”¼ì²˜'),
    Patch(facecolor='#FFA500', label='ìƒí˜¸ì‘ìš© í”¼ì²˜'),
    Patch(facecolor='#4ECDC4', label='ê¸°ë³¸ í”¼ì²˜')
]
plt.legend(handles=legend_elements, loc='lower right')

# ê°’ í‘œì‹œ
for i, (idx, row) in enumerate(top_features.iterrows()):
    plt.text(row['importance'], i, f" {row['importance']:.4f}", 
             va='center', fontsize=9)

plt.tight_layout()
plt.savefig('models/feature_importance_enhanced.png', dpi=300, bbox_inches='tight')
print(f"   âœ… ì €ì¥: models/feature_importance_enhanced.png")
plt.close()

# 10. í•™ìŠµ ê³¡ì„ 
print(f"\n10. í•™ìŠµ ê³¡ì„  ìƒì„± ì¤‘...")
print("   (ì´ ê³¼ì •ì€ ëª‡ ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤...)")

sample_size = min(100000, len(X_train))
if len(X_train) > sample_size:
    print(f"   ìƒ˜í”Œë§: {len(X_train):,} â†’ {sample_size:,}ê°œ")
    indices = np.random.choice(len(X_train), sample_size, replace=False)
    X_train_sample = X_train.iloc[indices]
    y_train_sample = y_train.iloc[indices]
else:
    X_train_sample = X_train
    y_train_sample = y_train

train_sizes = np.linspace(0.1, 1.0, 10)
train_sizes_abs, train_scores, val_scores = learning_curve(
    model, X_train_sample, y_train_sample,
    train_sizes=train_sizes,
    cv=3,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42
)

train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
val_mean = np.mean(val_scores, axis=1)
val_std = np.std(val_scores, axis=1)

plt.figure(figsize=(10, 6))
plt.plot(train_sizes_abs, train_mean, 'o-', color='#4ECDC4', 
         label='í•™ìŠµ ì •í™•ë„', linewidth=2, markersize=8)
plt.fill_between(train_sizes_abs, train_mean - train_std, train_mean + train_std, 
                 alpha=0.2, color='#4ECDC4')

plt.plot(train_sizes_abs, val_mean, 'o-', color='#FF6B6B', 
         label='ê²€ì¦ ì •í™•ë„', linewidth=2, markersize=8)
plt.fill_between(train_sizes_abs, val_mean - val_std, val_mean + val_std, 
                 alpha=0.2, color='#FF6B6B')

plt.xlabel('í•™ìŠµ ìƒ˜í”Œ ìˆ˜', fontsize=12)
plt.ylabel('ì •í™•ë„', fontsize=12)
plt.title('í•™ìŠµ ê³¡ì„  (Learning Curve)', fontsize=16, fontweight='bold', pad=20)
plt.legend(loc='lower right', fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('models/learning_curve_enhanced.png', dpi=300, bbox_inches='tight')
print(f"   âœ… ì €ì¥: models/learning_curve_enhanced.png")
plt.close()

# 11. ëª¨ë¸ ì €ì¥
print(f"\n11. ëª¨ë¸ ì €ì¥ ì¤‘...")

model_data = {
    'model': model,
    'feature_names': feature_columns,
    'model_type': 'RandomForestClassifier',
    'train_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
    'train_accuracy': train_accuracy,
    'test_accuracy': test_accuracy,
    'train_f1': train_f1,
    'test_f1': test_f1,
    'note': 'ì—­ ì¹´í…Œê³ ë¦¬ í”¼ì²˜ + ìƒí˜¸ì‘ìš© í”¼ì²˜ í¬í•¨ (ì—…ë¬´ì§€êµ¬, ìƒì—…ì§€êµ¬, í™˜ìŠ¹ì—­ë“±ê¸‰, ëŒ€í•™ê°€, ì£¼ê±°ì§€ì—­)'
}

model_path = 'models/subway_congestion_model_enhanced.pkl'
with open(model_path, 'wb') as f:
    pickle.dump(model_data, f)

print(f"   âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")

# 12. ìµœì¢… ìš”ì•½
print(f"\n" + "=" * 80)
print(f"âœ… í•™ìŠµ ì™„ë£Œ!")
print(f"=" * 80)
print(f"ğŸ“Š ìµœì¢… ê²°ê³¼:")
print(f"   ëª¨ë¸: Random Forest (100 trees)")
print(f"   í”¼ì²˜ ìˆ˜: {len(feature_columns)}ê°œ")
print(f"   - ì—­ ì¹´í…Œê³ ë¦¬: {len(category_features)}ê°œ")
print(f"   - ìƒí˜¸ì‘ìš©: {len(interaction_features)}ê°œ")
print(f"   í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_accuracy*100:.2f}%")
print(f"   í…ŒìŠ¤íŠ¸ F1-Score: {test_f1:.4f}")
print(f"   ê³¼ì í•©: {overfit*100:.2f}%p")
print(f"\nì €ì¥ëœ íŒŒì¼:")
print(f"   ğŸ“ ëª¨ë¸: {model_path}")
print(f"   ğŸ“Š í˜¼ë™í–‰ë ¬: models/confusion_matrix_enhanced.png")
print(f"   ğŸ“Š íŠ¹ì„± ì¤‘ìš”ë„: models/feature_importance_enhanced.png")
print(f"   ğŸ“Š í•™ìŠµ ê³¡ì„ : models/learning_curve_enhanced.png")
print(f"\në‹¤ìŒ ë‹¨ê³„: main.pyì—ì„œ ëª¨ë¸ ê²½ë¡œë¥¼ '{model_path}'ë¡œ ë³€ê²½")
print("=" * 80)
