"""
ì§€í•˜ì²  í˜¼ì¡ë„ ì˜ˆì¸¡ - ê°œì„ ëœ ëª¨ë¸ í•™ìŠµ ëª¨ë“ˆ (ê³¼ì í•© í•´ê²°)
subway-congestion-prediction/src/model_trainer_improved.py

ê°œì„  ì‚¬í•­:
1. ì •ê·œí™” ê°•í™” (min_samples_split, min_samples_leaf ì¦ê°€)
2. íŠ¸ë¦¬ ê¹Šì´ ì œí•œ ì™„í™” â†’ ìë™ ì„ íƒ
3. íŠ¹ì„± ìƒ˜í”Œë§ ì¶”ê°€ (max_features)
4. ì•™ìƒë¸” í¬ê¸° ì¦ê°€
5. ê²€ì¦ ë°ì´í„° ë¶„ë¦¬
6. Early Stopping ê°œë… ì ìš©
7. í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì¡°ì •
"""

import pandas as pd
import numpy as np
import pickle
import os
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, 
    classification_report, 
    confusion_matrix,
    f1_score,
    precision_score,
    recall_score
)

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False


class ImprovedModelTrainer:
    """ê³¼ì í•©ì„ ë°©ì§€í•œ ê°œì„ ëœ Random Forest ëª¨ë¸ í•™ìŠµ í´ë˜ìŠ¤"""
    
    def __init__(self, feature_data_path, model_save_path):
        """
        Args:
            feature_data_path: í”¼ì²˜ ë°ì´í„° ê²½ë¡œ
            model_save_path: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        """
        self.feature_data_path = feature_data_path
        self.model_save_path = model_save_path
        
        os.makedirs(model_save_path, exist_ok=True)
        
        self.df = None
        self.X_train = None
        self.X_val = None
        self.X_test = None
        self.y_train = None
        self.y_val = None
        self.y_test = None
        self.model = None
        self.feature_names = None
        self.scaler = None
        
    def load_feature_data(self, filename='subway_features_balanced.csv'):  # âœ… Balanced!
        """í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ëœ ë°ì´í„° ë¡œë“œ"""
        filepath = os.path.join(self.feature_data_path, filename)
        
        print(f"ğŸ“‚ í”¼ì²˜ ë°ì´í„° ë¡œë”© ì¤‘: {filepath}")
        
        self.df = pd.read_csv(filepath, encoding='utf-8-sig')
        
        print(f"âœ… ë¡œë”© ì™„ë£Œ!")
        print(f"   - í–‰ ìˆ˜: {len(self.df):,}")
        print(f"   - ì—´ ìˆ˜: {len(self.df.columns)}")
        
        return self.df
    
    def prepare_data(self, test_size=0.2, val_size=0.1, random_state=42):
        """í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬ (3-way split)"""
        print("\nğŸ“Š ë°ì´í„° ì¤€ë¹„ ì¤‘...")
        
        # íƒ€ê²Ÿ ë³€ìˆ˜
        target_column = 'í˜¼ì¡ë„ë ˆë²¨'
        
        # ì œì™¸í•  ì»¬ëŸ¼ë“¤
        exclude_columns = [
            target_column, 
            'ì‚¬ìš©ì¼ì', 
            'ì§€í•˜ì² ì—­', 
            'í˜¸ì„ ëª…', 
            'ì´ìŠ¹í•˜ì°¨ì¸ì›',  # íƒ€ê²Ÿ ëˆ„ì¶œ ë°©ì§€
            'í˜¼ì¡ë„',  # íƒ€ê²Ÿ ëˆ„ì¶œ ë°©ì§€
            'Unnamed: 0'
        ]
        
        # í”¼ì²˜ ì„ íƒ
        feature_columns = [col for col in self.df.columns if col not in exclude_columns]
        
        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        if self.df[feature_columns].isnull().sum().sum() > 0:
            print("âš ï¸  ê²°ì¸¡ì¹˜ ë°œê²¬! ì¤‘ì•™ê°’ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤...")
            self.df[feature_columns] = self.df[feature_columns].fillna(
                self.df[feature_columns].median()
            )
        
        # X, y ë¶„ë¦¬
        X = self.df[feature_columns].copy()
        y = self.df[target_column].copy()
        
        # ë¨¼ì € train+val / test ë¶„ë¦¬
        X_temp, self.X_test, y_temp, self.y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state, stratify=y
        )
        
        # train / val ë¶„ë¦¬
        val_ratio = val_size / (1 - test_size)
        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(
            X_temp, y_temp, test_size=val_ratio, random_state=random_state, stratify=y_temp
        )
        
        self.feature_names = feature_columns
        
        print(f"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!")
        print(f"   - í•™ìŠµ ë°ì´í„°: {len(self.X_train):,}ê°œ ({len(self.X_train)/len(X)*100:.1f}%)")
        print(f"   - ê²€ì¦ ë°ì´í„°: {len(self.X_val):,}ê°œ ({len(self.X_val)/len(X)*100:.1f}%)")
        print(f"   - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(self.X_test):,}ê°œ ({len(self.X_test)/len(X)*100:.1f}%)")
        print(f"   - í”¼ì²˜ ìˆ˜: {len(feature_columns)}ê°œ")
        
        # í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
        print(f"\ní´ë˜ìŠ¤ ë¶„í¬:")
        for label, count in sorted(y.value_counts().items()):
            print(f"   ë ˆë²¨ {label}: {count:,}ê°œ ({count/len(y)*100:.1f}%)")
        
        return self.X_train, self.X_val, self.X_test, self.y_train, self.y_val, self.y_test
    
    def train_model(self, model_type='random_forest', random_state=42):
        """
        ê³¼ì í•©ì„ ë°©ì§€í•œ ëª¨ë¸ í•™ìŠµ
        
        Args:
            model_type: 'random_forest' ë˜ëŠ” 'gradient_boosting'
        """
        print(f"\nğŸŒ² {model_type.upper()} ëª¨ë¸ í•™ìŠµ ì¤‘...")
        
        if model_type == 'random_forest':
            # ğŸ”¥ ê³¼ì í•© ë°©ì§€ ì„¤ì •
            self.model = RandomForestClassifier(
                n_estimators=200,           # íŠ¸ë¦¬ ìˆ˜ ì¦ê°€ (ì•ˆì •ì„±)
                max_depth=15,               # ê¹Šì´ ì œí•œ (ê³¼ì í•© ë°©ì§€)
                min_samples_split=100,      # ë¶„í•  ìµœì†Œ ìƒ˜í”Œ ì¦ê°€ â¬†ï¸
                min_samples_leaf=50,        # ë¦¬í”„ ìµœì†Œ ìƒ˜í”Œ ì¦ê°€ â¬†ï¸
                max_features='sqrt',        # í”¼ì²˜ ìƒ˜í”Œë§ (ë‹¤ì–‘ì„±)
                max_samples=0.8,            # ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œ ë¹„ìœ¨
                class_weight='balanced',    # í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬
                random_state=random_state,
                n_jobs=-1,
                verbose=1,
                oob_score=True              # Out-of-bag ì ìˆ˜
            )
        
        elif model_type == 'gradient_boosting':
            # Gradient Boosting (ëŒ€ì•ˆ)
            self.model = GradientBoostingClassifier(
                n_estimators=100,
                learning_rate=0.05,         # ë‚®ì€ í•™ìŠµë¥ 
                max_depth=5,                # ì–•ì€ íŠ¸ë¦¬
                min_samples_split=100,
                min_samples_leaf=50,
                subsample=0.8,              # ìƒ˜í”Œ ì„œë¸Œìƒ˜í”Œë§
                random_state=random_state,
                verbose=1
            )
        
        # í•™ìŠµ
        print("\ní•™ìŠµ ì‹œì‘...")
        self.model.fit(self.X_train, self.y_train)
        
        if hasattr(self.model, 'oob_score_'):
            print(f"\nğŸ“Š Out-of-Bag Score: {self.model.oob_score_:.4f}")
        
        print("âœ… í•™ìŠµ ì™„ë£Œ!")
        
        return self.model
    
    def evaluate_model(self, show_validation=True):
        """ëª¨ë¸ í‰ê°€ (Train/Val/Test)"""
        print("\nğŸ“ˆ ëª¨ë¸ í‰ê°€ ì¤‘...")
        
        # ì˜ˆì¸¡
        y_train_pred = self.model.predict(self.X_train)
        y_val_pred = self.model.predict(self.X_val)
        y_test_pred = self.model.predict(self.X_test)
        
        # ì§€í‘œ ê³„ì‚°
        train_acc = accuracy_score(self.y_train, y_train_pred)
        val_acc = accuracy_score(self.y_val, y_val_pred)
        test_acc = accuracy_score(self.y_test, y_test_pred)
        
        train_f1 = f1_score(self.y_train, y_train_pred, average='weighted')
        val_f1 = f1_score(self.y_val, y_val_pred, average='weighted')
        test_f1 = f1_score(self.y_test, y_test_pred, average='weighted')
        
        print("\n" + "="*70)
        print("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œ")
        print("="*70)
        
        print(f"\nğŸ¯ ì •í™•ë„ (Accuracy):")
        print(f"   - í•™ìŠµ ë°ì´í„°:   {train_acc:.4f} ({train_acc*100:.2f}%)")
        print(f"   - ê²€ì¦ ë°ì´í„°:   {val_acc:.4f} ({val_acc*100:.2f}%)")
        print(f"   - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_acc:.4f} ({test_acc*100:.2f}%)")
        
        # ğŸ”¥ ê³¼ì í•© ì§„ë‹¨
        overfitting_gap = train_acc - val_acc
        print(f"\nâš ï¸  ê³¼ì í•© ì •ë„: {overfitting_gap:.4f} ({overfitting_gap*100:.2f}%p)")
        if overfitting_gap > 0.05:
            print("   âŒ ê³¼ì í•© ì˜ì‹¬! (5%p ì´ìƒ ì°¨ì´)")
        elif overfitting_gap > 0.02:
            print("   âš ï¸  ì•½ê°„ì˜ ê³¼ì í•© (2~5%p ì°¨ì´)")
        else:
            print("   âœ… ì •ìƒ ë²”ìœ„ (2%p ì´í•˜ ì°¨ì´)")
        
        print(f"\nğŸ“Š F1-Score:")
        print(f"   - í•™ìŠµ ë°ì´í„°:   {train_f1:.4f}")
        print(f"   - ê²€ì¦ ë°ì´í„°:   {val_f1:.4f}")
        print(f"   - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_f1:.4f}")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„¸ ë¦¬í¬íŠ¸
        print("\n" + "="*70)
        print("ğŸ“‹ ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸ (í…ŒìŠ¤íŠ¸ ë°ì´í„°)")
        print("="*70)
        print("\ní˜¼ì¡ë„ ë ˆë²¨: 0=ì—¬ìœ , 1=ë³´í†µ, 2=í˜¼ì¡, 3=ë§¤ìš°í˜¼ì¡")
        print()
        print(classification_report(
            self.y_test, y_test_pred,
            target_names=['ì—¬ìœ ', 'ë³´í†µ', 'í˜¼ì¡', 'ë§¤ìš°í˜¼ì¡'],
            digits=4
        ))
        
        return {
            'train_acc': train_acc,
            'val_acc': val_acc,
            'test_acc': test_acc,
            'train_f1': train_f1,
            'val_f1': val_f1,
            'test_f1': test_f1,
            'overfitting_gap': overfitting_gap,
            'y_test_pred': y_test_pred
        }
    
    def plot_confusion_matrix(self, y_test_pred, save_path='models'):
        """í˜¼ë™ í–‰ë ¬ ì‹œê°í™”"""
        print("\nğŸ“Š í˜¼ë™ í–‰ë ¬ ìƒì„± ì¤‘...")
        
        cm = confusion_matrix(self.y_test, y_test_pred)
        
        # ì •ê·œí™”ëœ í˜¼ë™ í–‰ë ¬ (ë¹„ìœ¨)
        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        
        # ë‘ ê°œ ê·¸ë˜í”„ ìƒì„±
        fig, axes = plt.subplots(1, 2, figsize=(16, 6))
        
        # 1. ì ˆëŒ€ê°’
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],
                   xticklabels=['ì—¬ìœ ', 'ë³´í†µ', 'í˜¼ì¡', 'ë§¤ìš°í˜¼ì¡'],
                   yticklabels=['ì—¬ìœ ', 'ë³´í†µ', 'í˜¼ì¡', 'ë§¤ìš°í˜¼ì¡'])
        axes[0].set_title('í˜¼ë™ í–‰ë ¬ (ê°œìˆ˜)', fontsize=14, pad=15)
        axes[0].set_ylabel('ì‹¤ì œ ê°’', fontsize=11)
        axes[0].set_xlabel('ì˜ˆì¸¡ ê°’', fontsize=11)
        
        # 2. ë¹„ìœ¨
        sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Oranges', ax=axes[1],
                   xticklabels=['ì—¬ìœ ', 'ë³´í†µ', 'í˜¼ì¡', 'ë§¤ìš°í˜¼ì¡'],
                   yticklabels=['ì—¬ìœ ', 'ë³´í†µ', 'í˜¼ì¡', 'ë§¤ìš°í˜¼ì¡'])
        axes[1].set_title('í˜¼ë™ í–‰ë ¬ (ë¹„ìœ¨)', fontsize=14, pad=15)
        axes[1].set_ylabel('ì‹¤ì œ ê°’', fontsize=11)
        axes[1].set_xlabel('ì˜ˆì¸¡ ê°’', fontsize=11)
        
        plt.tight_layout()
        
        filepath = os.path.join(save_path, 'confusion_matrix_improved.png')
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        print(f"âœ… í˜¼ë™ í–‰ë ¬ ì €ì¥: {filepath}")
        
        plt.close()
        
        return cm
    
    def plot_feature_importance(self, top_n=20, save_path='models'):
        """íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”"""
        print(f"\nğŸ“Š ìƒìœ„ {top_n}ê°œ íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™” ì¤‘...")
        
        importances = self.model.feature_importances_
        feature_importance_df = pd.DataFrame({
            'feature': self.feature_names,
            'importance': importances
        }).sort_values('importance', ascending=False)
        
        top_features = feature_importance_df.head(top_n)
        
        plt.figure(figsize=(12, 8))
        colors = plt.cm.viridis(np.linspace(0, 1, len(top_features)))
        plt.barh(range(len(top_features)), top_features['importance'], color=colors)
        plt.yticks(range(len(top_features)), top_features['feature'])
        plt.xlabel('ì¤‘ìš”ë„', fontsize=12)
        plt.title(f'ìƒìœ„ {top_n}ê°œ íŠ¹ì„± ì¤‘ìš”ë„', fontsize=16, pad=20)
        plt.gca().invert_yaxis()
        plt.grid(axis='x', alpha=0.3)
        plt.tight_layout()
        
        filepath = os.path.join(save_path, 'feature_importance_improved.png')
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        print(f"âœ… íŠ¹ì„± ì¤‘ìš”ë„ ì €ì¥: {filepath}")
        
        plt.close()
        
        print("\nìƒìœ„ 15ê°œ ì¤‘ìš” íŠ¹ì„±:")
        for idx, row in feature_importance_df.head(15).iterrows():
            print(f"   {row['feature']}: {row['importance']:.4f}")
        
        return feature_importance_df
    
    def plot_learning_curve(self, save_path='models'):
        """í•™ìŠµ ê³¡ì„  ê·¸ë¦¬ê¸° (ê³¼ì í•© ì§„ë‹¨)"""
        print("\nğŸ“ˆ í•™ìŠµ ê³¡ì„  ìƒì„± ì¤‘...")
        
        from sklearn.model_selection import learning_curve
        
        train_sizes, train_scores, val_scores = learning_curve(
            self.model, 
            self.X_train, 
            self.y_train,
            cv=5,
            n_jobs=-1,
            train_sizes=np.linspace(0.1, 1.0, 10),
            scoring='accuracy'
        )
        
        train_mean = np.mean(train_scores, axis=1)
        train_std = np.std(train_scores, axis=1)
        val_mean = np.mean(val_scores, axis=1)
        val_std = np.std(val_scores, axis=1)
        
        plt.figure(figsize=(10, 6))
        plt.plot(train_sizes, train_mean, label='í•™ìŠµ ì ìˆ˜', color='blue', marker='o')
        plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, 
                         alpha=0.15, color='blue')
        plt.plot(train_sizes, val_mean, label='ê²€ì¦ ì ìˆ˜', color='red', marker='s')
        plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, 
                         alpha=0.15, color='red')
        
        plt.xlabel('í•™ìŠµ ë°ì´í„° í¬ê¸°', fontsize=12)
        plt.ylabel('ì •í™•ë„', fontsize=12)
        plt.title('í•™ìŠµ ê³¡ì„  (ê³¼ì í•© ì§„ë‹¨)', fontsize=14, pad=15)
        plt.legend(loc='lower right', fontsize=11)
        plt.grid(alpha=0.3)
        plt.tight_layout()
        
        filepath = os.path.join(save_path, 'learning_curve.png')
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        print(f"âœ… í•™ìŠµ ê³¡ì„  ì €ì¥: {filepath}")
        
        plt.close()
    
    def cross_validate(self, cv=5):
        """êµì°¨ ê²€ì¦"""
        print(f"\nğŸ”„ {cv}-Fold êµì°¨ ê²€ì¦ ì¤‘...")
        
        scores = cross_val_score(
            self.model, 
            self.X_train, 
            self.y_train,
            cv=cv, 
            scoring='accuracy', 
            n_jobs=-1
        )
        
        print(f"âœ… êµì°¨ ê²€ì¦ ì™„ë£Œ!")
        print(f"   - í‰ê·  ì •í™•ë„: {scores.mean():.4f} Â± {scores.std():.4f}")
        print(f"   - ê° Fold: {[f'{s:.4f}' for s in scores]}")
        
        return scores
    
    def save_model(self, filename='subway_congestion_model_improved.pkl'):
        """ëª¨ë¸ ì €ì¥"""
        filepath = os.path.join(self.model_save_path, filename)
        
        print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘: {filepath}")
        
        model_data = {
            'model': self.model,
            'feature_names': self.feature_names,
            'train_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'model_type': type(self.model).__name__
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ!")
        print(f"   - íŒŒì¼ í¬ê¸°: {os.path.getsize(filepath) / 1024**2:.2f} MB")
        
        return filepath


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸš‡ ì§€í•˜ì²  í˜¼ì¡ë„ ì˜ˆì¸¡ - ê°œì„ ëœ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    print("="*70)
    print("ğŸ“Œ ê³¼ì í•© ë°©ì§€ ì „ëµ:")
    print("   1. Train/Val/Test 3-way ë¶„ë¦¬")
    print("   2. ì •ê·œí™” ê°•í™” (min_samples ì¦ê°€)")
    print("   3. íŠ¸ë¦¬ ê¹Šì´ ì œí•œ")
    print("   4. í”¼ì²˜/ìƒ˜í”Œ ì„œë¸Œìƒ˜í”Œë§")
    print("   5. í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì¡°ì •")
    print("="*70)
    
    FEATURE_DATA_PATH = 'data/processed'
    MODEL_SAVE_PATH = 'models'
    
    trainer = ImprovedModelTrainer(FEATURE_DATA_PATH, MODEL_SAVE_PATH)
    
    try:
        # 1. ë°ì´í„° ë¡œë“œ
        trainer.load_feature_data('subway_features_balanced.csv')
        
        # 2. ë°ì´í„° ì¤€ë¹„ (Train 70% / Val 10% / Test 20%)
        trainer.prepare_data(test_size=0.2, val_size=0.1, random_state=42)
        
        # 3. ëª¨ë¸ í•™ìŠµ
        trainer.train_model(model_type='random_forest', random_state=42)
        
        # 4. ëª¨ë¸ í‰ê°€
        results = trainer.evaluate_model()
        
        # 5. êµì°¨ ê²€ì¦
        trainer.cross_validate(cv=5)
        
        # 6. ì‹œê°í™”
        trainer.plot_confusion_matrix(results['y_test_pred'], save_path=MODEL_SAVE_PATH)
        trainer.plot_feature_importance(top_n=20, save_path=MODEL_SAVE_PATH)
        trainer.plot_learning_curve(save_path=MODEL_SAVE_PATH)
        
        # 7. ëª¨ë¸ ì €ì¥
        trainer.save_model()
        
        print("\n" + "="*70)
        print("âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
        print("="*70)
        
        # ìµœì¢… ìš”ì•½
        print(f"\nğŸ“Š ìµœì¢… ì„±ëŠ¥ ìš”ì•½:")
        print(f"   - í…ŒìŠ¤íŠ¸ ì •í™•ë„: {results['test_acc']:.4f} ({results['test_acc']*100:.2f}%)")
        print(f"   - í…ŒìŠ¤íŠ¸ F1-Score: {results['test_f1']:.4f}")
        print(f"   - ê³¼ì í•© ì •ë„: {results['overfitting_gap']:.4f} ({results['overfitting_gap']*100:.2f}%p)")
        
        if results['overfitting_gap'] <= 0.02:
            print(f"   âœ… ê³¼ì í•© ì—†ìŒ! ëª¨ë¸ì´ ì˜ ì¼ë°˜í™”ë¨")
        elif results['overfitting_gap'] <= 0.05:
            print(f"   âš ï¸  ì•½ê°„ì˜ ê³¼ì í•©, í•˜ì§€ë§Œ ì‚¬ìš© ê°€ëŠ¥")
        else:
            print(f"   âŒ ê³¼ì í•© ë°œìƒ! í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì • í•„ìš”")
        
        print(f"\nì €ì¥ëœ íŒŒì¼:")
        print(f"   - ëª¨ë¸: {MODEL_SAVE_PATH}/subway_congestion_model_improved.pkl")
        print(f"   - í˜¼ë™ í–‰ë ¬: {MODEL_SAVE_PATH}/confusion_matrix_improved.png")
        print(f"   - íŠ¹ì„± ì¤‘ìš”ë„: {MODEL_SAVE_PATH}/feature_importance_improved.png")
        print(f"   - í•™ìŠµ ê³¡ì„ : {MODEL_SAVE_PATH}/learning_curve.png")
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
