"""
ì§€í•˜ì²  í˜¼ì¡ë„ ì˜ˆì¸¡ - ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ëª¨ë“ˆ
subway-congestion-prediction/src/model_trainer.py

ì£¼ìš” ê¸°ëŠ¥:
1. Random Forest ëª¨ë¸ í•™ìŠµ
2. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
3. ëª¨ë¸ í‰ê°€ (ì •í™•ë„, F1-score ë“±)
4. íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
5. í•™ìŠµëœ ëª¨ë¸ ì €ì¥
"""

import pandas as pd
import numpy as np
import pickle
import os
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, 
    classification_report, 
    confusion_matrix,
    f1_score,
    precision_score,
    recall_score
)

# í•œê¸€ í°íŠ¸ ì„¤ì • (Windows)
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False


class ModelTrainer:
    """Random Forest ëª¨ë¸ í•™ìŠµ í´ë˜ìŠ¤"""
    
    def __init__(self, feature_data_path, model_save_path):
        """
        Args:
            feature_data_path: í”¼ì²˜ ë°ì´í„° ê²½ë¡œ
            model_save_path: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        """
        self.feature_data_path = feature_data_path
        self.model_save_path = model_save_path
        
        # í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
        os.makedirs(model_save_path, exist_ok=True)
        
        self.df = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.model = None
        self.feature_names = None
        
    def load_feature_data(self, filename='subway_features.csv'):
        """í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ëœ ë°ì´í„° ë¡œë“œ"""
        filepath = os.path.join(self.feature_data_path, filename)
        
        print(f"ğŸ“‚ í”¼ì²˜ ë°ì´í„° ë¡œë”© ì¤‘: {filepath}")
        
        self.df = pd.read_csv(filepath, encoding='utf-8-sig')
        
        print(f"âœ… ë¡œë”© ì™„ë£Œ!")
        print(f"   - í–‰ ìˆ˜: {len(self.df):,}")
        print(f"   - ì—´ ìˆ˜: {len(self.df.columns)}")
        
        return self.df
    
    def prepare_data(self, test_size=0.2, random_state=42):
        """í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬"""
        print("\nğŸ“Š ë°ì´í„° ì¤€ë¹„ ì¤‘...")
        
        # íƒ€ê²Ÿ ë³€ìˆ˜
        target_column = 'í˜¼ì¡ë„ë ˆë²¨'
        
        # í•™ìŠµì— ì‚¬ìš©í•˜ì§€ ì•Šì„ ì»¬ëŸ¼ë“¤
        exclude_columns = [
            target_column, 'ì‚¬ìš©ì¼ì', 'ì§€í•˜ì² ì—­', 'í˜¸ì„ ëª…', 
            'ì´ìŠ¹í•˜ì°¨ì¸ì›', 'í˜¼ì¡ë„'  # íƒ€ê²Ÿê³¼ ì§ì ‘ ê´€ë ¨ëœ ì»¬ëŸ¼ ì œì™¸
        ]
        
        # í”¼ì²˜ ì»¬ëŸ¼ ì„ íƒ
        feature_columns = [col for col in self.df.columns if col not in exclude_columns]
        
        # ê²°ì¸¡ì¹˜ í™•ì¸ ë° ì²˜ë¦¬
        if self.df[feature_columns].isnull().sum().sum() > 0:
            print("âš ï¸  ê²°ì¸¡ì¹˜ ë°œê²¬! 0ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤...")
            self.df[feature_columns] = self.df[feature_columns].fillna(0)
        
        # X, y ë¶„ë¦¬
        X = self.df[feature_columns]
        y = self.df[target_column]
        
        # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state, stratify=y
        )
        
        self.feature_names = feature_columns
        
        print(f"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!")
        print(f"   - í•™ìŠµ ë°ì´í„°: {len(self.X_train):,}ê°œ")
        print(f"   - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(self.X_test):,}ê°œ")
        print(f"   - í”¼ì²˜ ìˆ˜: {len(feature_columns)}ê°œ")
        print(f"\n   í”¼ì²˜ ëª©ë¡:")
        for i, col in enumerate(feature_columns, 1):
            print(f"   {i}. {col}")
        
        return self.X_train, self.X_test, self.y_train, self.y_test
    
    def train_model(self, n_estimators=100, max_depth=20, random_state=42):
        """Random Forest ëª¨ë¸ í•™ìŠµ"""
        print("\nğŸŒ² Random Forest ëª¨ë¸ í•™ìŠµ ì¤‘...")
        print(f"   - n_estimators: {n_estimators}")
        print(f"   - max_depth: {max_depth}")
        
        # Random Forest ë¶„ë¥˜ ëª¨ë¸ ìƒì„±
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            min_samples_split=10,
            min_samples_leaf=5,
            random_state=random_state,
            n_jobs=-1,  # ëª¨ë“  CPU ì½”ì–´ ì‚¬ìš©
            verbose=1
        )
        
        # í•™ìŠµ
        print("\ní•™ìŠµ ì‹œì‘...")
        self.model.fit(self.X_train, self.y_train)
        
        print("âœ… í•™ìŠµ ì™„ë£Œ!")
        
        return self.model
    
    def evaluate_model(self):
        """ëª¨ë¸ í‰ê°€"""
        print("\nğŸ“ˆ ëª¨ë¸ í‰ê°€ ì¤‘...")
        
        # ì˜ˆì¸¡
        y_train_pred = self.model.predict(self.X_train)
        y_test_pred = self.model.predict(self.X_test)
        
        # ì •í™•ë„
        train_accuracy = accuracy_score(self.y_train, y_train_pred)
        test_accuracy = accuracy_score(self.y_test, y_test_pred)
        
        # F1-score
        train_f1 = f1_score(self.y_train, y_train_pred, average='weighted')
        test_f1 = f1_score(self.y_test, y_test_pred, average='weighted')
        
        # ì •ë°€ë„ (Precision)
        train_precision = precision_score(self.y_train, y_train_pred, average='weighted')
        test_precision = precision_score(self.y_test, y_test_pred, average='weighted')
        
        # ì¬í˜„ìœ¨ (Recall)
        train_recall = recall_score(self.y_train, y_train_pred, average='weighted')
        test_recall = recall_score(self.y_test, y_test_pred, average='weighted')
        
        print("\n" + "="*60)
        print("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œ")
        print("="*60)
        
        print(f"\nğŸ¯ ì •í™•ë„ (Accuracy):")
        print(f"   - í•™ìŠµ ë°ì´í„°: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)")
        print(f"   - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")
        
        print(f"\nğŸ“Š F1-Score:")
        print(f"   - í•™ìŠµ ë°ì´í„°: {train_f1:.4f}")
        print(f"   - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_f1:.4f}")
        
        print(f"\nğŸ¯ ì •ë°€ë„ (Precision):")
        print(f"   - í•™ìŠµ ë°ì´í„°: {train_precision:.4f}")
        print(f"   - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_precision:.4f}")
        
        print(f"\nğŸ¯ ì¬í˜„ìœ¨ (Recall):")
        print(f"   - í•™ìŠµ ë°ì´í„°: {train_recall:.4f}")
        print(f"   - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_recall:.4f}")
        
        # ë¶„ë¥˜ ë¦¬í¬íŠ¸
        print("\n" + "="*60)
        print("ğŸ“‹ ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸ (í…ŒìŠ¤íŠ¸ ë°ì´í„°)")
        print("="*60)
        print("\ní˜¼ì¡ë„ ë ˆë²¨:")
        print("0: ì—¬ìœ , 1: ë³´í†µ, 2: í˜¼ì¡, 3: ë§¤ìš°í˜¼ì¡")
        print()
        print(classification_report(self.y_test, y_test_pred, 
                                   target_names=['ì—¬ìœ ', 'ë³´í†µ', 'í˜¼ì¡', 'ë§¤ìš°í˜¼ì¡']))
        
        return {
            'train_accuracy': train_accuracy,
            'test_accuracy': test_accuracy,
            'train_f1': train_f1,
            'test_f1': test_f1,
            'y_test_pred': y_test_pred
        }
    
    def plot_confusion_matrix(self, y_test_pred, save_path='models'):
        """í˜¼ë™ í–‰ë ¬(Confusion Matrix) ì‹œê°í™”"""
        print("\nğŸ“Š í˜¼ë™ í–‰ë ¬ ìƒì„± ì¤‘...")
        
        # í˜¼ë™ í–‰ë ¬ ê³„ì‚°
        cm = confusion_matrix(self.y_test, y_test_pred)
        
        # ì‹œê°í™”
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=['ì—¬ìœ ', 'ë³´í†µ', 'í˜¼ì¡', 'ë§¤ìš°í˜¼ì¡'],
                   yticklabels=['ì—¬ìœ ', 'ë³´í†µ', 'í˜¼ì¡', 'ë§¤ìš°í˜¼ì¡'])
        plt.title('í˜¼ë™ í–‰ë ¬ (Confusion Matrix)', fontsize=16, pad=20)
        plt.ylabel('ì‹¤ì œ ê°’', fontsize=12)
        plt.xlabel('ì˜ˆì¸¡ ê°’', fontsize=12)
        plt.tight_layout()
        
        # ì €ì¥
        filepath = os.path.join(save_path, 'confusion_matrix.png')
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        print(f"âœ… í˜¼ë™ í–‰ë ¬ ì €ì¥: {filepath}")
        
        plt.close()
        
        return cm
    
    def plot_feature_importance(self, top_n=20, save_path='models'):
        """íŠ¹ì„± ì¤‘ìš”ë„(Feature Importance) ì‹œê°í™”"""
        print(f"\nğŸ“Š ìƒìœ„ {top_n}ê°œ íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™” ì¤‘...")
        
        # íŠ¹ì„± ì¤‘ìš”ë„ ì¶”ì¶œ
        importances = self.model.feature_importances_
        feature_importance_df = pd.DataFrame({
            'feature': self.feature_names,
            'importance': importances
        }).sort_values('importance', ascending=False)
        
        # ìƒìœ„ Nê°œë§Œ ì„ íƒ
        top_features = feature_importance_df.head(top_n)
        
        # ì‹œê°í™”
        plt.figure(figsize=(12, 8))
        plt.barh(range(len(top_features)), top_features['importance'])
        plt.yticks(range(len(top_features)), top_features['feature'])
        plt.xlabel('ì¤‘ìš”ë„', fontsize=12)
        plt.title(f'ìƒìœ„ {top_n}ê°œ íŠ¹ì„± ì¤‘ìš”ë„', fontsize=16, pad=20)
        plt.gca().invert_yaxis()
        plt.tight_layout()
        
        # ì €ì¥
        filepath = os.path.join(save_path, 'feature_importance.png')
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        print(f"âœ… íŠ¹ì„± ì¤‘ìš”ë„ ì €ì¥: {filepath}")
        
        plt.close()
        
        # ìƒìœ„ 10ê°œ ì¶œë ¥
        print("\nìƒìœ„ 10ê°œ ì¤‘ìš” íŠ¹ì„±:")
        for i, row in feature_importance_df.head(10).iterrows():
            print(f"   {row['feature']}: {row['importance']:.4f}")
        
        return feature_importance_df
    
    def cross_validate(self, cv=5):
        """êµì°¨ ê²€ì¦"""
        print(f"\nğŸ”„ {cv}-Fold êµì°¨ ê²€ì¦ ì¤‘...")
        
        scores = cross_val_score(self.model, self.X_train, self.y_train, 
                                cv=cv, scoring='accuracy', n_jobs=-1)
        
        print(f"âœ… êµì°¨ ê²€ì¦ ì™„ë£Œ!")
        print(f"   - í‰ê·  ì •í™•ë„: {scores.mean():.4f} ({scores.mean()*100:.2f}%)")
        print(f"   - í‘œì¤€í¸ì°¨: {scores.std():.4f}")
        print(f"   - ê° Fold ì ìˆ˜: {scores}")
        
        return scores
    
    def save_model(self, filename='subway_congestion_model.pkl'):
        """í•™ìŠµëœ ëª¨ë¸ ì €ì¥"""
        filepath = os.path.join(self.model_save_path, filename)
        
        print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘: {filepath}")
        
        # ëª¨ë¸ê³¼ í”¼ì²˜ ì •ë³´ë¥¼ í•¨ê»˜ ì €ì¥
        model_data = {
            'model': self.model,
            'feature_names': self.feature_names,
            'train_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ!")
        print(f"   - íŒŒì¼ í¬ê¸°: {os.path.getsize(filepath) / 1024**2:.2f} MB")
        
        return filepath
    
    def hyperparameter_tuning(self, param_grid=None):
        """í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (ì„ íƒì‚¬í•­)"""
        print("\nğŸ”§ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì¤‘...")
        print("âš ï¸  ì´ ì‘ì—…ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        if param_grid is None:
            param_grid = {
                'n_estimators': [50, 100, 150],
                'max_depth': [10, 20, 30],
                'min_samples_split': [5, 10, 15],
                'min_samples_leaf': [2, 5, 10]
            }
        
        grid_search = GridSearchCV(
            RandomForestClassifier(random_state=42, n_jobs=-1),
            param_grid,
            cv=3,
            scoring='accuracy',
            verbose=2,
            n_jobs=-1
        )
        
        grid_search.fit(self.X_train, self.y_train)
        
        print("\nâœ… íŠœë‹ ì™„ë£Œ!")
        print(f"   - ìµœì  íŒŒë¼ë¯¸í„°: {grid_search.best_params_}")
        print(f"   - ìµœì  ì ìˆ˜: {grid_search.best_score_:.4f}")
        
        self.model = grid_search.best_estimator_
        
        return grid_search.best_params_


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸš‡ ì§€í•˜ì²  í˜¼ì¡ë„ ì˜ˆì¸¡ - ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    print("="*60)
    
    # ê²½ë¡œ ì„¤ì •
    FEATURE_DATA_PATH = 'data/processed'
    MODEL_SAVE_PATH = 'models'
    
    # ëª¨ë¸ íŠ¸ë ˆì´ë„ˆ ê°ì²´ ìƒì„±
    trainer = ModelTrainer(FEATURE_DATA_PATH, MODEL_SAVE_PATH)
    
    try:
        # 1. ë°ì´í„° ë¡œë“œ
        trainer.load_feature_data()
        
        # 2. ë°ì´í„° ì¤€ë¹„
        trainer.prepare_data(test_size=0.2, random_state=42)
        
        # 3. ëª¨ë¸ í•™ìŠµ
        trainer.train_model(n_estimators=100, max_depth=20, random_state=42)
        
        # 4. ëª¨ë¸ í‰ê°€
        results = trainer.evaluate_model()
        
        # 5. êµì°¨ ê²€ì¦
        trainer.cross_validate(cv=5)
        
        # 6. í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
        trainer.plot_confusion_matrix(results['y_test_pred'], save_path=MODEL_SAVE_PATH)
        
        # 7. íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”
        trainer.plot_feature_importance(top_n=20, save_path=MODEL_SAVE_PATH)
        
        # 8. ëª¨ë¸ ì €ì¥
        trainer.save_model()
        
        print("\n" + "="*60)
        print("âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
        print("="*60)
        print(f"\nì €ì¥ëœ íŒŒì¼:")
        print(f"   - ëª¨ë¸: {MODEL_SAVE_PATH}/subway_congestion_model.pkl")
        print(f"   - í˜¼ë™ í–‰ë ¬: {MODEL_SAVE_PATH}/confusion_matrix.png")
        print(f"   - íŠ¹ì„± ì¤‘ìš”ë„: {MODEL_SAVE_PATH}/feature_importance.png")
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (ì„ íƒì‚¬í•­ - ì£¼ì„ ì²˜ë¦¬)
        # print("\nğŸ”§ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤)")
        # trainer.hyperparameter_tuning()
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
